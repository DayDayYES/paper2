# TD3 ç®—æ³•æ”¹è¿›è¯´æ˜

## ğŸ“‹ æ”¹è¿›æ€»ç»“

å·²å°†ç®—æ³•ä»åŸºç¡€TD3å‡çº§ä¸º**å®Œæ•´æ ‡å‡†çš„TD3å®ç°**ï¼Œå¹¶é›†æˆäº†**ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPERï¼‰**ã€‚

---

## âœ… TD3 æ ¸å¿ƒç‰¹æ€§ï¼ˆå…¨éƒ¨å®ç°ï¼‰

### 1. Twin Criticsï¼ˆåŒQç½‘ç»œï¼‰âœ…
**ä½œç”¨**ï¼šå‡å°‘Qå€¼è¿‡ä¼°è®¡

**å®ç°**ï¼š
```python
# ä¸¤ä¸ªç‹¬ç«‹çš„Criticç½‘ç»œ
self.critic_1 = CriticNetwork(...)
self.critic_2 = CriticNetwork(...)
self.target_critic_1 = CriticNetwork(...)
self.target_critic_2 = CriticNetwork(...)
```

### 2. Clipped Double Q-Learningï¼ˆæˆªæ–­åŒQå­¦ä¹ ï¼‰âœ…
**ä½œç”¨**ï¼šä½¿ç”¨ä¸¤ä¸ªQå€¼ä¸­çš„æœ€å°å€¼ï¼Œè¿›ä¸€æ­¥å‡å°‘è¿‡ä¼°è®¡

**å®ç°**ï¼š
```python
target_q1 = self.target_critic_1.forward(states_, target_actions)
target_q2 = self.target_critic_2.forward(states_, target_actions)
target_q = T.min(target_q1, target_q2)  # å–æœ€å°å€¼
```

### 3. Delayed Policy Updatesï¼ˆå»¶è¿Ÿç­–ç•¥æ›´æ–°ï¼‰âœ…
**ä½œç”¨**ï¼šé™ä½ç­–ç•¥æ›´æ–°é¢‘ç‡ï¼Œæé«˜ç¨³å®šæ€§

**å®ç°**ï¼š
```python
if self.learn_step % self.policy_delay == 0:
    # æ¯policy_delayæ­¥æ‰æ›´æ–°Actor
    self.actor.train()
    # ... æ›´æ–°Actor
```

### 4. Target Policy Smoothingï¼ˆç›®æ ‡ç­–ç•¥å¹³æ»‘ï¼‰âœ… **æ–°å¢**
**ä½œç”¨**ï¼šä¸ºç›®æ ‡åŠ¨ä½œæ·»åŠ å™ªå£°ï¼Œå¹³æ»‘ä»·å€¼ä¼°è®¡ï¼Œå‡å°‘æ–¹å·®

**å®ç°**ï¼š
```python
# ä¸ºç›®æ ‡åŠ¨ä½œæ·»åŠ è£å‰ªçš„å™ªå£°
noise = T.clamp(T.randn_like(target_actions) * 0.2, -0.5, 0.5)
target_actions = T.clamp(target_actions + noise, -1.0, 1.0)
```

---

## ğŸ¯ ä¸»è¦æ”¹è¿›ç‚¹

### æ”¹è¿›1ï¼šåŠ¨ä½œè£å‰ª âœ…
**é—®é¢˜**ï¼šæ¢ç´¢å™ªå£°å¯èƒ½ä½¿åŠ¨ä½œè¶…å‡ºæœ‰æ•ˆèŒƒå›´

**ä¿®æ”¹å‰**ï¼š
```python
mu_prime = mu + noise
return mu_prime.cpu().detach().numpy()[0]  # æ²¡æœ‰è£å‰ª
```

**ä¿®æ”¹å**ï¼š
```python
mu_prime = mu + noise
mu_prime = T.clamp(mu_prime, -1.0, 1.0)  # è£å‰ªåˆ°[-1, 1]
return mu_prime.cpu().detach().numpy()[0]
```

---

### æ”¹è¿›2ï¼šç›®æ ‡ç­–ç•¥å¹³æ»‘ âœ… **æ–°å¢**
**TD3çš„å…³é”®æŠ€å·§**ï¼šä¸ºç›®æ ‡ç­–ç•¥æ·»åŠ å™ªå£°

**å®ç°**ï¼š
```python
with T.no_grad():
    target_actions = self.target_actor.forward(states_)
    
    # æ·»åŠ è£å‰ªçš„å™ªå£°
    noise = T.clamp(
        T.randn_like(target_actions) * 0.2,  # å™ªå£°æ ‡å‡†å·®
        -0.5, 0.5  # å™ªå£°èŒƒå›´
    )
    target_actions = T.clamp(target_actions + noise, -1.0, 1.0)
```

---

### æ”¹è¿›3ï¼šæ­£ç¡®çš„CriticæŸå¤±è®¡ç®— âœ…
**é—®é¢˜**ï¼šåŸæ¥å°†ä¸¤ä¸ªCriticçš„æŸå¤±ç›¸åŠ ï¼Œä¸ç¬¦åˆæ ‡å‡†

**ä¿®æ”¹å‰**ï¼š
```python
critic_loss_1 = F.mse_loss(target, critic_value_1)
critic_loss_2 = F.mse_loss(target, critic_value_2)
self.q_loss = ISWeights * (critic_loss_1 + critic_loss_2)
self.q_loss.sum().backward()
```

**ä¿®æ”¹å**ï¼š
```python
# åˆ†åˆ«è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
critic_1_loss_elementwise = (target - current_q1) ** 2
critic_1_loss = (ISWeights_tensor * critic_1_loss_elementwise).mean()
critic_1_loss.backward(retain_graph=True)

critic_2_loss_elementwise = (target - current_q2) ** 2
critic_2_loss = (ISWeights_tensor * critic_2_loss_elementwise).mean()
critic_2_loss.backward()
```

**ä¼˜åŠ¿**ï¼š
- æ¯ä¸ªCriticç‹¬ç«‹æ›´æ–°ï¼Œé¿å…æ¢¯åº¦å¹²æ‰°
- æ­£ç¡®åº”ç”¨PERæƒé‡åˆ°æ¯ä¸ªæ ·æœ¬

---

### æ”¹è¿›4ï¼šæ”¹è¿›çš„PERä¼˜å…ˆçº§æ›´æ–° âœ…
**é—®é¢˜**ï¼šåªä½¿ç”¨ä¸€ä¸ªCriticçš„TDè¯¯å·®

**ä¿®æ”¹å‰**ï¼š
```python
self.abs_errors = T.abs(target - critic_value_1)  # åªç”¨critic_1
self.replay_buffer.batch_update(tree_idx, self.abs_errors.detach().numpy())
```

**ä¿®æ”¹å**ï¼š
```python
# è®¡ç®—ä¸¤ä¸ªCriticçš„TDè¯¯å·®
td_error_1 = T.abs(target - current_q1)
td_error_2 = T.abs(target - current_q2)

# ä½¿ç”¨è¾ƒå¤§çš„TDè¯¯å·®ï¼ˆæ›´ä¿å®ˆçš„ä¼°è®¡ï¼‰
td_errors = T.max(td_error_1, td_error_2).detach()

# æ›´æ–°ä¼˜å…ˆçº§
self.replay_buffer.batch_update(valid_tree_idx, td_errors.cpu().numpy().flatten())
```

**ä¼˜åŠ¿**ï¼š
- ä½¿ç”¨ä¸¤ä¸ªCriticä¸­è¾ƒå¤§çš„è¯¯å·®ï¼Œæ›´å‡†ç¡®åæ˜ æ ·æœ¬é‡è¦æ€§
- åªæ›´æ–°æœ‰æ•ˆæ ·æœ¬çš„ä¼˜å…ˆçº§

---

### æ”¹è¿›5ï¼šæ¢¯åº¦è£å‰ª âœ… **æ–°å¢**
**ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

**å®ç°**ï¼š
```python
# Critic 1
critic_1_loss.backward(retain_graph=True)
T.nn.utils.clip_grad_norm_(self.critic_1.parameters(), max_norm=1.0)
self.critic_1.optimizer.step()

# Critic 2
critic_2_loss.backward()
T.nn.utils.clip_grad_norm_(self.critic_2.parameters(), max_norm=1.0)
self.critic_2.optimizer.step()

# Actor
actor_loss.backward()
T.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
self.actor.optimizer.step()
```

---

### æ”¹è¿›6ï¼šæ›´å¥½çš„æ•°æ®éªŒè¯ âœ…
**æ”¹è¿›**ï¼šæ›´ä¸¥æ ¼çš„æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥

**å®ç°**ï¼š
```python
valid_indices = []  # è®°å½•æœ‰æ•ˆæ ·æœ¬çš„ç´¢å¼•

for idx in range(len(batch_memory)):
    if not isinstance(batch_memory[idx], (list, tuple)) or len(batch_memory[idx]) < 5:
        continue
    
    states.append(batch_memory[idx][0])
    # ...
    valid_indices.append(idx)

# åªæ›´æ–°æœ‰æ•ˆæ ·æœ¬çš„ä¼˜å…ˆçº§
if len(valid_indices) > 0:
    valid_tree_idx = tree_idx[valid_indices]
    self.replay_buffer.batch_update(valid_tree_idx, td_errors.cpu().numpy().flatten())
```

---

### æ”¹è¿›7ï¼šä½¿ç”¨ `with T.no_grad()` âœ…
**ä½œç”¨**ï¼šè®¡ç®—ç›®æ ‡å€¼æ—¶ä¸éœ€è¦æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜

**å®ç°**ï¼š
```python
with T.no_grad():
    target_actions = self.target_actor.forward(states_)
    # ... è®¡ç®—ç›®æ ‡Qå€¼
    target = rewards + self.gamma * target_q
```

---

### æ”¹è¿›8ï¼šæ”¹è¿›çš„è½¯æ›´æ–°æ—¶æœº âœ…
**ä¿®æ”¹**ï¼šåªåœ¨Actoræ›´æ–°æ—¶æ‰è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ

**ä¿®æ”¹å‰**ï¼š
```python
# æ¯æ¬¡learnéƒ½æ›´æ–°
self.update_network_parameters()
```

**ä¿®æ”¹å**ï¼š
```python
if self.learn_step % self.policy_delay == 0:
    # Actoræ›´æ–°
    # ...
    # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
    self.update_network_parameters()
```

---

## ğŸ“Š å®Œæ•´çš„TD3+PERæµç¨‹

```
1. ä»PERç¼“å†²åŒºé‡‡æ · (å¸¦é‡è¦æ€§æƒé‡)
   â†“
2. è®¡ç®—ç›®æ ‡Qå€¼
   - ç›®æ ‡åŠ¨ä½œ = target_actor(s') + è£å‰ªå™ªå£°  [Target Policy Smoothing]
   - target_Q = min(Q1_target, Q2_target)      [Clipped Double Q]
   â†“
3. æ›´æ–°ä¸¤ä¸ªCritic
   - ä½¿ç”¨PERæƒé‡åŠ æƒæŸå¤±
   - åˆ†åˆ«åå‘ä¼ æ’­
   - æ¢¯åº¦è£å‰ª
   â†“
4. å»¶è¿Ÿæ›´æ–°Actor (æ¯policy_delayæ­¥)
   - æœ€å¤§åŒ–Q1(s, actor(s))
   - æ¢¯åº¦è£å‰ª
   - è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
   â†“
5. æ›´æ–°PERä¼˜å…ˆçº§
   - ä½¿ç”¨max(TD_error1, TD_error2)
```

---

## ğŸ”§ è¶…å‚æ•°å»ºè®®

### TD3 ç‰¹å®šå‚æ•°
```python
policy_delay = 2        # Actoræ›´æ–°å»¶è¿Ÿï¼ˆæ¨è2-3ï¼‰
target_noise = 0.2      # ç›®æ ‡ç­–ç•¥å™ªå£°æ ‡å‡†å·®
noise_clip = 0.5        # å™ªå£°è£å‰ªèŒƒå›´
action_noise = 0.1-0.2  # æ¢ç´¢å™ªå£°ï¼ˆè®­ç»ƒåˆæœŸå¯å¤§ä¸€äº›ï¼‰
```

### PER å‚æ•°
```python
alpha = 0.6            # ä¼˜å…ˆçº§æŒ‡æ•°ï¼ˆ0=å‡åŒ€é‡‡æ ·ï¼Œ1=å®Œå…¨ä¼˜å…ˆï¼‰
beta = 0.4             # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°ï¼ˆé€æ¸å¢åŠ åˆ°1ï¼‰
beta_increment = 0.001 # betaå¢é•¿ç‡
```

### ç½‘ç»œæ›´æ–°å‚æ•°
```python
tau = 0.005           # è½¯æ›´æ–°ç³»æ•°
gamma = 0.99          # æŠ˜æ‰£å› å­
batch_size = 64       # æ‰¹å¤§å°
learning_rate_actor = 0.0001    # Actorå­¦ä¹ ç‡
learning_rate_critic = 0.001    # Criticå­¦ä¹ ç‡
```

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ

### ç¨³å®šæ€§æå‡
- âœ… æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- âœ… ç›®æ ‡ç­–ç•¥å¹³æ»‘ï¼šå‡å°‘Qå€¼æ–¹å·®
- âœ… å»¶è¿Ÿæ›´æ–°ï¼šé™ä½ç­–ç•¥æŒ¯è¡

### æ€§èƒ½æå‡
- âœ… åŒQç½‘ç»œï¼šå‡å°‘Qå€¼è¿‡ä¼°è®¡
- âœ… PERï¼šä¼˜å…ˆå­¦ä¹ é‡è¦æ ·æœ¬
- âœ… æ­£ç¡®çš„æŸå¤±è®¡ç®—ï¼šæ›´æœ‰æ•ˆçš„å­¦ä¹ 

### é²æ£’æ€§æå‡
- âœ… åŠ¨ä½œè£å‰ªï¼šç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´
- âœ… æ•°æ®éªŒè¯ï¼šè·³è¿‡æ— æ•ˆæ ·æœ¬
- âœ… ç»´åº¦æ£€æŸ¥ï¼šé¿å…ç»´åº¦ä¸åŒ¹é…

---

## ğŸ§ª æµ‹è¯•å»ºè®®

### 1. éªŒè¯æ”¹è¿›
```python
# è¿è¡Œå¿«é€Ÿæµ‹è¯•
python quick_start.py

# è¿è¡Œå®Œæ•´è®­ç»ƒ
python train_dude.py
```

### 2. ç›‘æ§æŒ‡æ ‡
- **Critic Loss**ï¼šåº”è¯¥é€æ¸ä¸‹é™
- **Actor Loss**ï¼šå¯èƒ½æ³¢åŠ¨ï¼Œä½†æ€»ä½“è¶‹åŠ¿æ”¹å–„
- **TD Error**ï¼šåº”è¯¥é€æ¸å‡å°
- **Reward**ï¼šåº”è¯¥é€æ¸å¢åŠ 

### 3. å¯¹æ¯”å®éªŒ
å»ºè®®å¯¹æ¯”ä»¥ä¸‹é…ç½®ï¼š
- æœ‰/æ— ç›®æ ‡ç­–ç•¥å¹³æ»‘
- ä¸åŒçš„policy_delayå€¼ï¼ˆ1, 2, 3ï¼‰
- ä¸åŒçš„å™ªå£°å‚æ•°

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### è®­ç»ƒæ—¶ï¼ˆæ·»åŠ å™ªå£°ï¼‰
```python
action = agent.choose_action(observation, add_noise=True)
```

### æµ‹è¯•æ—¶ï¼ˆä¸æ·»åŠ å™ªå£°ï¼‰
```python
action = agent.choose_action(observation, add_noise=False)
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **åˆå§‹æ¢ç´¢**ï¼šè®­ç»ƒåˆæœŸå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å™ªå£°ï¼ˆ0.2-0.3ï¼‰
2. **åæœŸæ”¶æ•›**ï¼šè®­ç»ƒåæœŸå¯ä»¥é™ä½å™ªå£°ï¼ˆ0.05-0.1ï¼‰
3. **æ‰¹å¤§å°**ï¼šç¡®ä¿æ‰¹å¤§å°è¶³å¤Ÿå¤§ï¼ˆå»ºè®®â‰¥64ï¼‰
4. **ç»éªŒç§¯ç´¯**ï¼šå»ºè®®ç§¯ç´¯ä¸€å®šç»éªŒåå†å¼€å§‹å­¦ä¹ ï¼ˆå¦‚1000æ­¥ï¼‰

---

## ğŸ‰ æ€»ç»“

### æ ¸å¿ƒæ”¹è¿›
1. âœ… **å®Œæ•´TD3å®ç°**ï¼šæ‰€æœ‰4ä¸ªå…³é”®æŠ€å·§
2. âœ… **ä¼˜åŒ–PERé›†æˆ**ï¼šæ­£ç¡®çš„ä¼˜å…ˆçº§æ›´æ–°
3. âœ… **å¢å¼ºç¨³å®šæ€§**ï¼šæ¢¯åº¦è£å‰ªã€åŠ¨ä½œè£å‰ª
4. âœ… **æ”¹è¿›æŸå¤±è®¡ç®—**ï¼šç‹¬ç«‹æ›´æ–°ä¸¤ä¸ªCritic
5. âœ… **æ›´å¥½çš„ä»£ç è´¨é‡**ï¼šå®Œæ•´æ³¨é‡Šã€é”™è¯¯å¤„ç†

### ç¬¦åˆæ ‡å‡†
- âœ… ç¬¦åˆTD3è®ºæ–‡åŸå§‹å®ç°
- âœ… ç¬¦åˆPERè®ºæ–‡æ ‡å‡†
- âœ… å·¥ä¸šçº§ä»£ç è´¨é‡

ç°åœ¨æ‚¨çš„ç®—æ³•æ˜¯**æ ‡å‡†ã€å®Œæ•´ã€å¥å£®çš„TD3+PERå®ç°**ï¼ğŸš€

