"""
å¿«é€Ÿå¼€å§‹è„šæœ¬ - å¿«é€Ÿæµ‹è¯•æ–°ç¯å¢ƒå’Œè®­ç»ƒæµç¨‹
è¿è¡Œå°‘é‡å›åˆä»¥éªŒè¯ä»£ç æ­£ç¡®æ€§
"""
import os
import sys
import numpy as np

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import EnvironmentConfig, TrainingConfig
from uav_env_dude import UAVEnvDUDe
from perddpg_torch import Agent


def quick_test():
    """å¿«é€Ÿæµ‹è¯•ç¯å¢ƒå’Œè®­ç»ƒ"""
    print("="*70)
    print(" "*20 + "å¿«é€Ÿå¼€å§‹æµ‹è¯•")
    print("="*70)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    print("\n[1/5] åˆ›å»ºç¯å¢ƒ...")
    env = UAVEnvDUDe(
        EnvironmentConfig.UE_CLUSTER_1,
        EnvironmentConfig.UE_CLUSTER_2,
        num_uavs=2
    )
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  - UAVæ•°é‡: {env.uav_num}")
    print(f"  - UEæ•°é‡: {env.ue_num}")
    print(f"  - è§‚å¯Ÿç»´åº¦: {env.get_obs_dim()}")
    print(f"  - åŠ¨ä½œç»´åº¦: {env.get_action_dim()}")
    
    # 2. åˆ›å»ºæ™ºèƒ½ä½“
    print("\n[2/5] åˆ›å»ºæ™ºèƒ½ä½“...")
    test_batch_size = 16  # ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤§å°ç”¨äºå¿«é€Ÿæµ‹è¯•
    agents = []
    for i in range(env.uav_num):
        agent = Agent(
            alpha=TrainingConfig.ALPHA,
            beta=TrainingConfig.BETA,
            input_dims=env.get_obs_dim(),
            tau=TrainingConfig.TAU,
            n_actions=env.get_action_dim(),
            gamma=TrainingConfig.GAMMA,
            max_size=10000,  # å‡å°ç¼“å†²åŒºç”¨äºæµ‹è¯•
            C_fc1_dims=TrainingConfig.CRITIC_FC1_DIMS,
            C_fc2_dims=TrainingConfig.CRITIC_FC2_DIMS,
            C_fc3_dims=TrainingConfig.CRITIC_FC3_DIMS,
            A_fc1_dims=TrainingConfig.ACTOR_FC1_DIMS,
            A_fc2_dims=TrainingConfig.ACTOR_FC2_DIMS,
            batch_size=test_batch_size,  # ä½¿ç”¨æµ‹è¯•æ‰¹å¤§å°
            n_agents=env.uav_num,
            noise=TrainingConfig.NOISE
        )
        agents.append(agent)
    print(f"âœ“ åˆ›å»º {len(agents)} ä¸ªæ™ºèƒ½ä½“ (batch_size={test_batch_size})")
    
    # 3. æµ‹è¯•å•æ­¥æ‰§è¡Œ
    print("\n[3/5] æµ‹è¯•å•æ­¥æ‰§è¡Œ...")
    obs = env.reset()
    print(f"  - åˆå§‹çŠ¶æ€: {[o.shape for o in obs]}")
    
    actions = []
    for i, agent in enumerate(agents):
        action = agent.choose_action(obs[i])
        action = np.clip(action, -1, 1)
        actions.append(action)
    print(f"  - åŠ¨ä½œå½¢çŠ¶: {[a.shape for a in actions]}")
    
    obs_, reward, done, info = env.step(actions)
    print(f"  - æ–°çŠ¶æ€: {[o.shape for o in obs_]}")
    print(f"  - å¥–åŠ±: {reward:.2f}")
    print(f"  - é€Ÿç‡: {info['total_rate']:.2f} Mbps")
    print(f"  - åŠŸç‡: {info['total_power']:.2f} W")
    print("âœ“ å•æ­¥æ‰§è¡ŒæˆåŠŸ")
    
    # 4. æµ‹è¯•è®­ç»ƒå¾ªç¯
    print("\n[4/5] æµ‹è¯•è®­ç»ƒå¾ªç¯ (5ä¸ªå›åˆ)...")
    test_episodes = 5
    test_timestamp = 10
    total_steps = 0
    
    for episode in range(test_episodes):
        obs = env.reset()
        episode_reward = 0
        episode_rate = 0
        
        for t in range(test_timestamp):
            # é€‰æ‹©åŠ¨ä½œ
            actions = []
            for i, agent in enumerate(agents):
                action = agent.choose_action(obs[i])
                action = np.clip(action, -1, 1)
                actions.append(action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs_, reward, done, info = env.step(actions)
            
            # å­˜å‚¨ç»éªŒ
            for i, agent in enumerate(agents):
                agent.remember((obs[i], actions[i], reward, obs_[i], done))
            
            total_steps += 1
            
            # å­¦ä¹ ï¼ˆç¼“å†²åŒºè¶³å¤Ÿå¤§æ—¶ï¼Œå¹¶ä½¿ç”¨æ­£ç¡®çš„æ‰¹å¤§å°ï¼‰
            if total_steps > test_batch_size * 2:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç»éªŒ
                for agent in agents:
                    agent.learn(num_iteration=1, batch_size1=test_batch_size)
            
            obs = obs_
            episode_reward += reward
            episode_rate += info['total_rate']
        
        avg_rate = episode_rate / test_timestamp
        print(f"  Episode {episode+1}: Reward={episode_reward:7.2f}, "
              f"Avg Rate={avg_rate:6.2f} Mbps")
    
    print("âœ“ è®­ç»ƒå¾ªç¯æµ‹è¯•æˆåŠŸ")
    
    # 5. æµ‹è¯•ç¯å¢ƒç‰¹æ€§
    print("\n[5/5] æµ‹è¯•ç¯å¢ƒç‰¹æ€§...")
    
    # æµ‹è¯•Gumbel-Softmaxå…³è”
    test_logits = [np.random.randn(env.ue_num * 2) for _ in range(env.uav_num)]
    b_ul, b_dl = env.parse_association_from_action(test_logits, hard=True)
    ul_assoc = np.argmax(b_ul, axis=0)
    dl_assoc = np.argmax(b_dl, axis=0)
    
    print(f"  - ä¸Šè¡Œå…³è”å‰5ä¸ªUE: {ul_assoc[:5]}")
    print(f"  - ä¸‹è¡Œå…³è”å‰5ä¸ªUE: {dl_assoc[:5]}")
    print(f"  - è§£è€¦ç”¨æˆ·æ•°: {np.sum(ul_assoc != dl_assoc)}/{env.ue_num}")
    
    # æµ‹è¯•æ¸©åº¦é€€ç«
    initial_temp = env.temperature
    for _ in range(100):
        env.step(actions)
    final_temp = env.temperature
    print(f"  - æ¸©åº¦é€€ç«: {initial_temp:.3f} â†’ {final_temp:.3f}")
    
    print("âœ“ ç¯å¢ƒç‰¹æ€§æµ‹è¯•æˆåŠŸ")
    
    # æ€»ç»“
    print("\n" + "="*70)
    print(" "*25 + "æµ‹è¯•é€šè¿‡ï¼")
    print("="*70)
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œå®Œæ•´è®­ç»ƒ: python train_dude.py")
    print("  2. ä¿®æ”¹é…ç½®æ–‡ä»¶: config.py")
    print("  3. æŸ¥çœ‹è®­ç»ƒæŒ‡å—: TRAINING_GUIDE.md")
    print("\nç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€")


if __name__ == '__main__':
    try:
        quick_test()
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

